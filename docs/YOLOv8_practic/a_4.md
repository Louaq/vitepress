---
title: Ê®°ÂûãËí∏È¶è
top: 1
sticky: 999
---

## ‰∫å„ÄÅËí∏È¶èÊïôÁ®ã

Áü•ËØÜËí∏È¶èÁöÑ‰∏ªË¶ÅÊñπÊ≥ïÂèØ‰ª•ÂàÜ‰∏∫‰∏âÁßçÔºöÂü∫‰∫éÂìçÂ∫îÁöÑÁü•ËØÜËí∏È¶èÔºàÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫ÊàñÂØπÊúÄÁªàÈ¢ÑÊµãÁöÑÊ®°‰ªøÔºâ„ÄÅÂü∫‰∫éÁâπÂæÅÁöÑÁü•ËØÜËí∏È¶èÔºà‰ΩøÁî®ÊïôÂ∏àÊ®°Âûã‰∏≠Èó¥Â±ÇÁöÑÁâπÂæÅË°®Á§∫Ôºâ‰ª•ÂèäÂü∫‰∫éÂÖ≥Á≥ªÁöÑÁü•ËØÜËí∏È¶èÔºàÂà©Áî®Ê®°ÂûãÂÜÖÈÉ®‰∏çÂêåÂ±ÇÊàñ‰∏çÂêåÊï∞ÊçÆÁÇπ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºâ„ÄÇÊØèÁßçÊñπÊ≥ïÈÉΩÊó®Âú®‰ªéÂ§ßÊ®°Âûã‰∏≠ÊèêÂèñÊúâÊïà‰ø°ÊÅØÔºåÂπ∂ÈÄöËøáÁâπÂÆöÁöÑÊçüÂ§±ÂáΩÊï∞Â∞ÜËøô‰∫õ‰ø°ÊÅØÁÅåËæìÁªôÂ≠¶ÁîüÊ®°Âûã‚Äã„ÄÇ

È¶ñÂÖàÔºåÂü∫‰∫éÊ®°ÂûãÁöÑÁü•ËØÜËí∏È¶èÁ±ªÂûãÂåÖÊã¨Ôºö

-   1\. Âü∫‰∫éÂìçÂ∫îÁöÑËí∏È¶èÔºàResponse-basedÔºâÔºö‰ΩøÁî®ÊïôÂ∏àÊ®°ÂûãÁöÑÊúÄÂêéËæìÂá∫Â±ÇÁöÑ‰ø°ÊÅØÔºàÂ¶ÇÁ±ªÂà´Ê¶ÇÁéáÔºâÊù•ËÆ≠ÁªÉÂ≠¶ÁîüÊ®°Âûã„ÄÇ
-   2\. Âü∫‰∫éÁâπÂæÅÁöÑËí∏È¶èÔºàFeature-basedÔºâÔºöÂà©Áî®ÊïôÂ∏àÊ®°ÂûãÁöÑ‰∏≠Èó¥Â±ÇÁâπÂæÅÊù•ÊåáÂØºÂ≠¶ÁîüÊ®°Âûã„ÄÇ
-   3\. Âü∫‰∫éÂÖ≥Á≥ªÁöÑËí∏È¶èÔºàRelation-basedÔºâÔºö‰æßÈáç‰∫éÊïôÂ∏àÊ®°ÂûãÂÜÖ‰∏çÂêåÁâπÂæÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÂ¶ÇÁâπÂæÅÂõæ‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇ

Ëí∏È¶èËøáÁ®ãÁöÑÂÆûÊñΩÊñπÂºèÔºö

-   1\. Âú®Á∫øËí∏È¶èÔºàOnline distillationÔºâÔºöÊïôÂ∏àÊ®°ÂûãÂíåÂ≠¶ÁîüÊ®°ÂûãÂêåÊó∂ËÆ≠ÁªÉÔºåÂ≠¶ÁîüÊ®°ÂûãÂÆûÊó∂Â≠¶‰π†ÊïôÂ∏àÊ®°ÂûãÁöÑÁü•ËØÜ„ÄÇ
-   2\. Á¶ªÁ∫øËí∏È¶èÔºàOffline distillationÔºâÔºöÂÖàËÆ≠ÁªÉÊïôÂ∏àÊ®°ÂûãÔºåÂÜç‰ΩøÁî®ËØ•Ê®°ÂûãÊù•ËÆ≠ÁªÉÂ≠¶ÁîüÊ®°ÂûãÔºåÂ≠¶ÁîüÊ®°Âûã‰∏ç‰ºöÂΩ±ÂìçÊïôÂ∏àÊ®°Âûã„ÄÇ
-   3\. Ëá™Ëí∏È¶èÔºàSelf distillationÔºâÔºöÊ®°Âûã‰ΩøÁî®Ëá™Â∑±ÁöÑÈ¢ÑÊµã‰Ωú‰∏∫ËΩØÊ†áÁ≠æÊù•ÊèêÈ´òËá™Â∑±ÁöÑÊÄßËÉΩ„ÄÇ

Áü•ËØÜËí∏È¶èÊòØ‰∏Ä‰∏™Â§öÊ†∑ÂåñÁöÑÈ¢ÜÂüüÔºåÂåÖÊã¨ÂêÑÁßç‰∏çÂêåÁöÑÊñπÊ≥ïÊù•‰ºòÂåñÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÊÄßËÉΩÂíåÂ§ßÂ∞è„ÄÇ‰ªéÂü∫Êú¨ÁöÑÂü∫‰∫éÂìçÂ∫î„ÄÅÁâπÂæÅÂíåÂÖ≥Á≥ªÁöÑËí∏È¶èÔºåÂà∞Êõ¥È´òÁ∫ßÁöÑÂú®Á∫ø„ÄÅÁ¶ªÁ∫øÂíåËá™Ëí∏È¶èËøáÁ®ãÔºåÂÜçÂà∞ÁâπÂÆöÁöÑÊäÄÊúØÂ¶ÇÂØπÊäóÊÄßËí∏È¶èÊàñÈáèÂåñËí∏È¶èÁ≠âÔºåÊØè‰∏ÄÁßçÊñπÊ≥ïÈÉΩÊó®Âú®Ëß£ÂÜ≥‰∏çÂêåÁöÑÈóÆÈ¢òÂíåÈúÄÊ±Ç„ÄÇ


* * *

**üëëÊ≠£Âºè‰øÆÊîπÊïôÁ®ãüëë**

* * *

### 2.1 ‰øÆÊîπ‰∏Ä

‰∏ãÈù¢ÁªôÂá∫‰∫Ü‰∏ÄÊÆµ‰ª£Á†ÅÔºåÊàë‰ª¨Â∞ÜËøôÊÆµ‰ª£Á†ÅÊâæÂà∞ÁõÆÂΩï'ultralytics/utils'‰∏ãÂàõÂª∫‰∏Ä‰∏™.pyÊñá‰ª∂Â≠òÊîæËøõÂéªÔºåÊñá‰ª∂ÁöÑÂêçÂ≠óÊàë‰ª¨ÂëΩÂêç‰∏∫AddLoss.pyÔºåÂàõÂª∫Â•ΩÁöÑÊñá‰ª∂Â¶Ç‰∏ãÂõæÊâÄÁ§∫->

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/9dae7af60581fc0b71e12ea3fda49fcf.png)‚Äã

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
 
def is_parallel(model):
    """Returns True if model is of type DP or DDP."""
    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))
 
def de_parallel(model):
    """De-parallelize a model: returns single-GPU model if model is of type DP or DDP."""
    return model.module if is_parallel(model) else model
 
 
class MimicLoss(nn.Module):
    def __init__(self, channels_s, channels_t):
        super(MimicLoss, self).__init__()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.mse = nn.MSELoss()
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
            losses.append(self.mse(s, t))
        loss = sum(losses)
        return loss
 
 
class CWDLoss(nn.Module):
    """PyTorch version of `Channel-wise Distillation for Semantic Segmentation.
    <https://arxiv.org/abs/2011.13256>`_.
    """
 
    def __init__(self, channels_s, channels_t, tau=1.0):
        super(CWDLoss, self).__init__()
        self.tau = tau
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
 
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
 
            N, C, H, W = s.shape
 
            # normalize in channel diemension
            softmax_pred_T = F.softmax(t.view(-1, W * H) / self.tau, dim=1)  # [N*C, H*W]
 
            logsoftmax = torch.nn.LogSoftmax(dim=1)
            cost = torch.sum(
                softmax_pred_T * logsoftmax(t.view(-1, W * H) / self.tau) -
                softmax_pred_T * logsoftmax(s.view(-1, W * H) / self.tau)) * (self.tau ** 2)
 
            losses.append(cost / (C * N))
        loss = sum(losses)
 
        return loss
 
 
class MGDLoss(nn.Module):
    def __init__(self, channels_s, channels_t, alpha_mgd=0.00002, lambda_mgd=0.65):
        super(MGDLoss, self).__init__()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.alpha_mgd = alpha_mgd
        self.lambda_mgd = lambda_mgd
 
        self.generation = [
            nn.Sequential(
                nn.Conv2d(channel, channel, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(channel, channel, kernel_size=3, padding=1)).to(device) for channel in channels_t
        ]
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
            losses.append(self.get_dis_loss(s, t, idx) * self.alpha_mgd)
        loss = sum(losses)
        return loss
 
    def get_dis_loss(self, preds_S, preds_T, idx):
        loss_mse = nn.MSELoss(reduction='sum')
        N, C, H, W = preds_T.shape
 
        device = preds_S.device
        mat = torch.rand((N, 1, H, W)).to(device)
        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)
 
        masked_fea = torch.mul(preds_S, mat)
        new_fea = self.generation[idx](masked_fea)
        dis_loss = loss_mse(new_fea, preds_T) / N
        return dis_loss
 
 
class Distill_LogitLoss:
    def __init__(self, p, t_p, alpha=0.25):
        t_ft = torch.cuda.FloatTensor if t_p[0].is_cuda else torch.Tensor
        self.p = p
        self.t_p = t_p
        self.logit_loss = t_ft([0])
        self.DLogitLoss = nn.MSELoss(reduction="none")
        self.bs = p[0].shape[0]
        self.alpha = alpha
 
    def __call__(self):
        # per output
        assert len(self.p) == len(self.t_p)
        for i, (pi, t_pi) in enumerate(zip(self.p, self.t_p)):  # layer index, layer predictions
            assert pi.shape == t_pi.shape
            self.logit_loss += torch.mean(self.DLogitLoss(pi, t_pi))
        return self.logit_loss[0] * self.alpha
 
 
def get_fpn_features(x, model, fpn_layers=[15, 18, 21]):
    y, fpn_feats = [], []
    with torch.no_grad():
        model = de_parallel(model)
        module_list = model.model[:-1] if hasattr(model, "model") else model[:-1]
        for m in module_list:
            # if not from previous layer
            if m.f != -1:
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            x = m(x)
            y.append(x if m.i in model.save else None)  # save output
            if m.i in fpn_layers:
                fpn_feats.append(x)
    return fpn_feats
 
def get_channels(model, fpn_layers=[15, 18, 21]):
    y, out_channels = [], []
    p = next(model.parameters())
    x = torch.zeros((1, 3, 64, 64), device=p.device)
    with torch.no_grad():
        model = de_parallel(model)
        module_list = model.model[:-1] if hasattr(model, "model") else model[:-1]
 
        for m in module_list:
            # if not from previous layer
            if m.f != -1:
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            x = m(x)
            y.append(x if m.i in model.save else None)  # save output
            if m.i in fpn_layers:
                out_channels.append(x.shape[1])
    return out_channels
 
 
class FeatureLoss(nn.Module):
    def __init__(self, channels_s, channels_t, distiller='cwd'):
        super(FeatureLoss, self).__init__()
 
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.align_module = nn.ModuleList([
            nn.Conv2d(channel, tea_channel, kernel_size=1, stride=1, padding=0).to(device)
            for channel, tea_channel in zip(channels_s, channels_t)
        ])
        self.norm = [
            nn.BatchNorm2d(tea_channel, affine=False).to(device)
            for tea_channel in channels_t
        ]
 
        if distiller == 'mimic':
            self.feature_loss = MimicLoss(channels_s, channels_t)
 
        elif distiller == 'mgd':
            self.feature_loss = MGDLoss(channels_s, channels_t)
 
        elif distiller == 'cwd':
            self.feature_loss = CWDLoss(channels_s, channels_t)
        else:
            raise NotImplementedError
 
    def forward(self, y_s, y_t):
        assert len(y_s) == len(y_t)
        tea_feats = []
        stu_feats = []
 
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            s = self.align_module[idx](s)
            s = self.norm[idx](s)
            t = self.norm[idx](t)
            tea_feats.append(t)
            stu_feats.append(s)
 
        loss = self.feature_loss(stu_feats, tea_feats)
        return loss
```

* * *

### 2.2 ‰øÆÊîπ‰∫å

‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊàë‰ª¨ÊâæÂà∞Êñá‰ª∂'ultralytics/engine/trainer.py'ÊåâÁÖßÊàëÁöÑÂõæÁâáÂÜÖÂÆπÂ§çÂà∂Á≤òË¥¥Âà∞ÊåáÂÆö‰ΩçÁΩÆÂç≥ÂèØÔºÅ**Ê†πÊçÆ‰∏ãÂõæËøõË°å‰øÆÊîπ->**

**Âú®ËØ•Êñá‰ª∂ÁöÑÂºÄÂ§¥Êàë‰ª¨ÂÖàÊ∑ªÂä†‰∏§Ë°åÊ®°ÂùóÁöÑÂØºÂÖ•‰ª£Á†ÅÔºå**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/75c50bcf7da0b274a82980957568a607.png)

```python
from ultralytics.utils import IterableSimpleNamespace
from ultralytics.utils.AddLoss import get_fpn_features, Distill_LogitLoss, de_parallel, get_channels, FeatureLoss
```





![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/9e0dd4f62fb8bc7cf0d58207febd24d2.png)‚Äã

```python
        #------------------------------Add-Param-Start---------------
        self.featureloss = 0
        self.logitloss = 0
        self.teacherloss = 0
        self.distillloss =None
        self.model_t = overrides.get("model_t", None)
        self.distill_feat_type = "cwd"  # "cwd","mgd","mimic"
        self.distillonline = False  # False or True
        self.logit_loss = False # False or True
        self.distill_layers = [2, 4, 6, 8, 12, 15, 18, 21]
        # ------------------------------Add-Param-End-----------------
```

* * *

### 2.3 ‰øÆÊîπ‰∏â

ÊåâÁÖßÂõæÁâáËøõË°å‰øÆÊîπÂç≥ÂèØ„ÄÇ

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8a0944a19191b16fb6377031860965e5.png)‚Äã

```python
        if self.model_t is not None:
            for k, v in self.model_t.model.named_parameters():
                v.requires_grad = True
            self.model_t = self.model_t.to(self.device)
```

* * *

### 2.4 ‰øÆÊîπÂõõ¬†

**ÊåâÁÖßÂõæÁâáËøõË°å‰øÆÊîπÂç≥ÂèØ„ÄÇ**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/91c986bedf8396dde7eb0d932a4c4474.png)‚Äã

```python
            if self.model_t is not None:
                self.model_t = nn.parallel.DistributedDataParallel(self.model_t, device_ids=[RANK])
```



* * *

### 2.5 ‰øÆÊîπ‰∫î¬†

Ê≠§Â§ÑÁöÑ‰øÆÊîπÂíå‰∏äÈù¢Êúâ‰∫õ‰∏ç‰∏ÄÊ†∑Ôºå‰∏äÈù¢ÁöÑ‰ª£Á†ÅÈÉΩÊòØÊ∑ªÂä†ÔºåÊ≠§Â§ÑÁöÑ‰ª£Á†Å‰∏∫ÊõøÊç¢„ÄÇ

```
        self.optimizer = self.build_optimizer(model=self.model,
                                              model_t=self.model_t,
                                              distillloss=self.distillloss,
                                              distillonline=self.distillonline,
                                              name=self.args.optimizer,
                                              lr=self.args.lr0,
                                              momentum=self.args.momentum,
                                              decay=weight_decay,
                                              iterations=iterations)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/269369a15060c4a7e978184ecc5f72ca.png)‚Äã

* * *

### 2.6 ‰øÆÊîπÂÖ≠

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

```python
        self.model = de_parallel(self.model)
        if self.model_t is not None:
            self.model_t = de_parallel(self.model_t)
            self.channels_s = get_channels(self.model,self.distill_layers)
            self.channels_t = get_channels(self.model_t,self.distill_layers)
            self.distillloss = FeatureLoss(channels_s=self.channels_s, channels_t=self.channels_t, distiller= self.distill_feat_type)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bffa91ba199491eea639b992b54319f7.png)‚Äã

* * *

### 2.7 ‰øÆÊîπ‰∏É¬†

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

```python
            if self.model_t is not None:
                self.model_t.eval()
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ce70466a6fecf9192f58f54e010a35ed.png)‚Äã

* * *

### 2.8 ‰øÆÊîπÂÖ´¬†

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

```
                    pred_s= self.model(batch['img'])
                    stu_features = get_fpn_features(batch['img'], self.model,fpn_layers=self.distill_layers)
```



![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/396a0574ac2a105925cb7de38aa6b875.png)

* * *

### 2.8 ‰øÆÊîπ‰πù

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f824b6a1d28753225a30708809dfc7a0.png)‚Äã

```
                    if self.model_t is not None:
                        distill_weight = ((1 - math.cos(i * math.pi / len(self.train_loader))) / 2) * (0.1 - 1) + 1
                        with torch.no_grad():
                            pred_t_offline = self.model_t(batch['img'])
                            tea_features = get_fpn_features(batch['img'], self.model_t,
                                                            fpn_layers=self.distill_layers)  # forward
                            self.featureloss = self.distillloss(stu_features, tea_features) * distill_weight
                            self.loss += self.featureloss
 
                        if self.distillonline:
                            self.model_t.train()
                            pred_t_online = self.model_t(batch['img'])
                            for p in pred_t_online:
                                p = p.detach()
                            if i == 0 and epoch == 0:
                                self.model_t.args["box"] = self.model.args.box
                                self.model_t.args["cls"] = self.model.args.cls
                                self.model_t.args["dfl"] = self.model.args.dfl
                                self.model_t.args = IterableSimpleNamespace(**self.model_t.args)
                            self.teacherloss, _ = self.model_t(batch, pred_t_online)
 
                            if RANK != -1:
                                self.teacherloss *= world_size
                            self.loss += self.teacherloss
 
                        if self.logit_loss:
                            if not self.distillonline:
                                distill_logit = Distill_LogitLoss(pred_s, pred_t_offline)
                            else:
                                distill_logit = Distill_LogitLoss(pred_s, pred_t_online)
                            self.logitloss = distill_logit()
                            self.loss += self.logitloss
```



* * *

### 2.9 ‰øÆÊîπÂçÅ

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34fb82b4b0a045659a84dc210597c2c9.png)‚Äã

```
                mem = f"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G"  # (GB)
                loss_len = self.tloss.shape[0] if len(self.tloss.shape) else 1
                losses = self.tloss if loss_len > 1 else torch.unsqueeze(self.tloss, 0)
                if RANK in {-1, 0}:
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1
                    pbar.set_description(
                        ('%12s' * 2 + '%12.4g' * (5 + loss_length)) %
                        (f'{epoch + 1}/{self.epochs}', mem, * losses, self.featureloss, self.teacherloss, self.logitloss, batch['cls'].shape[0], batch['img'].shape[-1]))
                    self.run_callbacks("on_batch_end")
                    if self.args.plots and ni in self.plot_idx:
                        self.plot_training_samples(batch, ni)
```



* * *

### 2.10 ‰øÆÊîπÂçÅ‰∏Ä

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

```cobol
, model_t, distillloss, distillonline=False,
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/461f9e7eab2ec673c70a592e1b9880fb.png)‚Äã

* * *

### 2.11 ‰øÆÊîπÂçÅ‰∫å

**‰øÆÊîπÊïôÁ®ãÁúãÂõæÁâáÔºÅ**

```
        if model_t is not None and distillonline:
            for v in model_t.modules():
                # print(v)
                if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)
                    g[2].append(v.bias)
                if isinstance(v, bn):  # weight (no decay)
                    g[1].append(v.weight)
                elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
                    g[0].append(v.weight)
 
        if model_t is not None and distillloss is not None:
            for k, v in distillloss.named_modules():
                # print(v)
                if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)
                    g[2].append(v.bias)
                if isinstance(v, bn) or 'bn' in k:  # weight (no decay)
                    g[1].append(v.weight)
                elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
                    g[0].append(v.weight)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2aaa34487751a2a75b69e7ae11ebf041.png)‚Äã¬†

* * *

### 2.12 ‰øÆÊîπÂçÅ‰∏â

PSÔºöÊ≥®ÊÑèÊ≠§Â§ÑÊàë‰ª¨Êõ¥Êç¢‰∫Ü‰øÆÊîπÁöÑÊñá‰ª∂‰∫ÜÔºÅÔºÅ

Êàë‰ª¨ÊâæÂà∞Êñá‰ª∂'ultralytics/cfg/\_\_init\_\_.py'ÔºåÊåâÁÖßÊàëÁöÑÂõæÁâáËøõË°å‰øÆÊîπÔºÅ

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0cd5a2dfc2166bf627d902dbeb110a7d.png)‚Äã

* * *

### 2.13 ‰øÆÊîπÂçÅÂõõ

PSÔºöÊ≥®ÊÑèÊ≠§Â§ÑÊàë‰ª¨Êõ¥Êç¢‰∫Ü‰øÆÊîπÁöÑÊñá‰ª∂‰∫ÜÔºÅÔºÅ

Êàë‰ª¨ÊâæÂà∞Êñá‰ª∂'ultralytics/models/[yolo](https://so.csdn.net/so/search?q=yolo&spm=1001.2101.3001.7020)/detect/train.py'ÊåâÁÖßÂõæÁâáËøõË°å‰øÆÊîπÂç≥ÂèØÔºÅ

```
        return ('\n' + '%12s' *
                (7 + len(self.loss_names))) % (
        'Epoch', 'GPU_mem', *self.loss_names, 'dfeaLoss', 'dlineLoss', 'dlogitLoss', 'Instances',
        'Size')
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/149cb45c94ef85a8a984fb67f1fb4a50.png)‚Äã

* * *

### 2.14 ‰øÆÊîπÂçÅ‰∫î

PSÔºöÊ≥®ÊÑèÊ≠§Â§ÑÊàë‰ª¨Êõ¥Êç¢‰∫Ü‰øÆÊîπÁöÑÊñá‰ª∂‰∫ÜÔºÅÔºÅ

Êàë‰ª¨ÊâæÂà∞Êñá‰ª∂'ultralytics/engine/model.py'ÊåâÁÖßÂõæÁâáËøõË°å‰øÆÊîπÂç≥ÂèØÔºÅ

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0e61fc3af46c27799e142c941ef55ed7.png)‚Äã¬†

**Âà∞Ê≠§Â§ÑÂ∞±‰øÆÊîπÂÆåÊàê‰∫ÜÔºåÂâ©‰∏ãÁöÑÂ∞±ÊòØÂ¶Ç‰Ωï‰ΩøÁî®ËøõË°åÊ®°ÂûãËí∏È¶è‰∫ÜÔºÅ**¬†

* * *

## ‰∏â„ÄÅ‰ΩøÁî®ÊïôÁ®ã¬†

Ê®°ÂûãËí∏È¶èÊåáÁöÑÊòØÔºöÁî®ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºàÊ≥®ÊÑèÊòØËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºâÂéªÊïôÂè¶‰∏Ä‰∏™Ê®°ÂûãÔºÅÔºàÂÖ∂‰∏≠ÊïôÂ∏àÊ®°ÂûãÂøÖÈ°ªÊòØËÆ≠ÁªÉÂ•ΩÁöÑÊùÉÈáçÔºåÂ≠¶ÁîüÊ®°ÂûãÂèØ‰ª•ÊòØyamlÊñá‰ª∂‰πüÂèØ‰ª•ÊòØÊùÉÈáçÊñá‰ª∂Âú∞ÂùÄÔºâ

Âú®ÂºÄÂßã‰πãÂâçÊàë‰ª¨ÈúÄË¶ÅÂáÜÂ§á‰∏Ä‰∏™ÊïôÂ∏àÊ®°ÂûãÔºåÊàë‰ª¨ËøôÈáåÂ∞±Áî®YOLOv8l‰∏∫‰æãÔºåÂÖ∂ÊùÉÈáçÊñá‰ª∂ÂèØ‰ª•ÂéªÂÆòÊñπ‰∏ãËΩΩ„ÄÇ¬†

### 3.1 Ê®°ÂûãËí∏È¶è‰ª£Á†Å

PSÔºöÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÁöÑÊ®°ÂûãÈÖçÁΩÆÊñá‰ª∂ÈúÄË¶Å‰øùÊåÅ‰∏ÄËá¥Ôºå‰πüÂ∞±ÊòØËØ¥‰Ω†Â≠¶ÁîüÊ®°ÂûãÂÅáËÆæÁî®‰∫ÜBiFPNÈÇ£‰πà‰Ω†ÁöÑÊïôÂ∏àÊ®°Âûã‰πüÈúÄË¶ÅÁî®BiFPNÂéªËÆ≠ÁªÉÂê¶ÂàôÂ∞±‰ºöÊä•ÈîôÔºÅ

```
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO
 
if __name__ == '__main__':
    model_t = YOLO(r'weights/yolov8l.pt')  # Ê≠§Â§ÑÂ°´ÂÜôÊïôÂ∏àÊ®°ÂûãÁöÑÊùÉÈáçÊñá‰ª∂Âú∞ÂùÄ
 
    model_t.model.model[-1].set_Distillation = True  # ‰∏çÁî®ÁêÜ‰ºöÊ≠§Â§ÑÁî®‰∫éËÆæÁΩÆÊ®°ÂûãËí∏È¶è
 
    model_s = YOLO(r'ultralytics/cfg/models/v8/yolov8.yaml')  # Â≠¶ÁîüÊñá‰ª∂ÁöÑyamlÊñá‰ª∂ or ÊùÉÈáçÊñá‰ª∂Âú∞ÂùÄ
 
    model_s.train(data=r'C:\Users\Administrator\Desktop\Snu77\ultralytics-main\New_GC-DET\data.yaml',  #  Â∞ÜdataÂêéÈù¢ÊõøÊç¢‰Ω†Ëá™Â∑±ÁöÑÊï∞ÊçÆÈõÜÂú∞ÂùÄ
                cache=False,
                imgsz=640,
                epochs=100,
                single_cls=False,  # ÊòØÂê¶ÊòØÂçïÁ±ªÂà´Ê£ÄÊµã
                batch=1,
                close_mosaic=10,
                workers=0,
                device='0',
                optimizer='SGD',  # using SGD
                amp=True,  # Â¶ÇÊûúÂá∫Áé∞ËÆ≠ÁªÉÊçüÂ§±‰∏∫NanÂèØ‰ª•ÂÖ≥Èó≠amp
                project='runs/train',
                name='exp',
                model_t=model_t.model
                )
```



* * *

### 3.2 ÂºÄÂßãËí∏È¶è¬†

Êàë‰ª¨Â∞ÜËí∏È¶èÁöÑ‰ª£Á†ÅÂ§çÂà∂Á≤òË¥¥Âà∞‰∏Ä‰∏™pyÊñá‰ª∂ÂÜÖÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫ÔºÅ

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b2c9311f52d08fc4d304239d309937a.png)‚Äã

Êàë‰ª¨ËøêË°åËí∏È¶èÁöÑpyÊñá‰ª∂Âç≥ÂèØÔºåÊ®°ÂûãÂ∞±‰ºöÂºÄÂßãËÆ≠ÁªÉÂπ∂‰∏îËí∏È¶è„ÄÇ‰∏ãÈù¢ÁöÑÂõæÁâáÂ∞±ÊòØÊ®°ÂûãÂºÄÂßãËÆ≠ÁªÉÂπ∂‰∏îËí∏È¶èÔºåÂèØ‰ª•ÁúãÂà∞Êàë‰ª¨ÂºÄÂêØ‰∫ÜËí∏È¶èÊçüÂ§±'dfeaLoss ¬† dlineLoss'

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/339ee686982c2e83b90cfd85e2890948.png)‚Äã¬†

