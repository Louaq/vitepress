---
title: æ¨¡å‹è’¸é¦
top: 1
sticky: 999
---

## äºŒã€è’¸é¦æ•™ç¨‹

çŸ¥è¯†è’¸é¦çš„ä¸»è¦æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸‰ç§ï¼šåŸºäºå“åº”çš„çŸ¥è¯†è’¸é¦ï¼ˆåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºæˆ–å¯¹æœ€ç»ˆé¢„æµ‹çš„æ¨¡ä»¿ï¼‰ã€åŸºäºç‰¹å¾çš„çŸ¥è¯†è’¸é¦ï¼ˆä½¿ç”¨æ•™å¸ˆæ¨¡å‹ä¸­é—´å±‚çš„ç‰¹å¾è¡¨ç¤ºï¼‰ä»¥åŠåŸºäºå…³ç³»çš„çŸ¥è¯†è’¸é¦ï¼ˆåˆ©ç”¨æ¨¡å‹å†…éƒ¨ä¸åŒå±‚æˆ–ä¸åŒæ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ï¼‰ã€‚æ¯ç§æ–¹æ³•éƒ½æ—¨åœ¨ä»å¤§æ¨¡å‹ä¸­æå–æœ‰æ•ˆä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„æŸå¤±å‡½æ•°å°†è¿™äº›ä¿¡æ¯çŒè¾“ç»™å­¦ç”Ÿæ¨¡å‹â€‹ã€‚

é¦–å…ˆï¼ŒåŸºäºæ¨¡å‹çš„çŸ¥è¯†è’¸é¦ç±»å‹åŒ…æ‹¬ï¼š

-   1\. åŸºäºå“åº”çš„è’¸é¦ï¼ˆResponse-basedï¼‰ï¼šä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„æœ€åè¾“å‡ºå±‚çš„ä¿¡æ¯ï¼ˆå¦‚ç±»åˆ«æ¦‚ç‡ï¼‰æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚
-   2\. åŸºäºç‰¹å¾çš„è’¸é¦ï¼ˆFeature-basedï¼‰ï¼šåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾æ¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ã€‚
-   3\. åŸºäºå…³ç³»çš„è’¸é¦ï¼ˆRelation-basedï¼‰ï¼šä¾§é‡äºæ•™å¸ˆæ¨¡å‹å†…ä¸åŒç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚ç‰¹å¾å›¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚

è’¸é¦è¿‡ç¨‹çš„å®æ–½æ–¹å¼ï¼š

-   1\. åœ¨çº¿è’¸é¦ï¼ˆOnline distillationï¼‰ï¼šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹åŒæ—¶è®­ç»ƒï¼Œå­¦ç”Ÿæ¨¡å‹å®æ—¶å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚
-   2\. ç¦»çº¿è’¸é¦ï¼ˆOffline distillationï¼‰ï¼šå…ˆè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå†ä½¿ç”¨è¯¥æ¨¡å‹æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œå­¦ç”Ÿæ¨¡å‹ä¸ä¼šå½±å“æ•™å¸ˆæ¨¡å‹ã€‚
-   3\. è‡ªè’¸é¦ï¼ˆSelf distillationï¼‰ï¼šæ¨¡å‹ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ä½œä¸ºè½¯æ ‡ç­¾æ¥æé«˜è‡ªå·±çš„æ€§èƒ½ã€‚

çŸ¥è¯†è’¸é¦æ˜¯ä¸€ä¸ªå¤šæ ·åŒ–çš„é¢†åŸŸï¼ŒåŒ…æ‹¬å„ç§ä¸åŒçš„æ–¹æ³•æ¥ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œå¤§å°ã€‚ä»åŸºæœ¬çš„åŸºäºå“åº”ã€ç‰¹å¾å’Œå…³ç³»çš„è’¸é¦ï¼Œåˆ°æ›´é«˜çº§çš„åœ¨çº¿ã€ç¦»çº¿å’Œè‡ªè’¸é¦è¿‡ç¨‹ï¼Œå†åˆ°ç‰¹å®šçš„æŠ€æœ¯å¦‚å¯¹æŠ—æ€§è’¸é¦æˆ–é‡åŒ–è’¸é¦ç­‰ï¼Œæ¯ä¸€ç§æ–¹æ³•éƒ½æ—¨åœ¨è§£å†³ä¸åŒçš„é—®é¢˜å’Œéœ€æ±‚ã€‚


* * *

**ğŸ‘‘æ­£å¼ä¿®æ”¹æ•™ç¨‹ğŸ‘‘**

* * *

### 2.1 ä¿®æ”¹ä¸€

ä¸‹é¢ç»™å‡ºäº†ä¸€æ®µä»£ç ï¼Œæˆ‘ä»¬å°†è¿™æ®µä»£ç æ‰¾åˆ°ç›®å½•'ultralytics/utils'ä¸‹åˆ›å»ºä¸€ä¸ª.pyæ–‡ä»¶å­˜æ”¾è¿›å»ï¼Œæ–‡ä»¶çš„åå­—æˆ‘ä»¬å‘½åä¸ºAddLoss.pyï¼Œåˆ›å»ºå¥½çš„æ–‡ä»¶å¦‚ä¸‹å›¾æ‰€ç¤º->

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/9dae7af60581fc0b71e12ea3fda49fcf.png)â€‹

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
 
def is_parallel(model):
    """Returns True if model is of type DP or DDP."""
    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))
 
def de_parallel(model):
    """De-parallelize a model: returns single-GPU model if model is of type DP or DDP."""
    return model.module if is_parallel(model) else model
 
 
class MimicLoss(nn.Module):
    def __init__(self, channels_s, channels_t):
        super(MimicLoss, self).__init__()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.mse = nn.MSELoss()
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
            losses.append(self.mse(s, t))
        loss = sum(losses)
        return loss
 
 
class CWDLoss(nn.Module):
    """PyTorch version of `Channel-wise Distillation for Semantic Segmentation.
    <https://arxiv.org/abs/2011.13256>`_.
    """
 
    def __init__(self, channels_s, channels_t, tau=1.0):
        super(CWDLoss, self).__init__()
        self.tau = tau
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
 
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
 
            N, C, H, W = s.shape
 
            # normalize in channel diemension
            softmax_pred_T = F.softmax(t.view(-1, W * H) / self.tau, dim=1)  # [N*C, H*W]
 
            logsoftmax = torch.nn.LogSoftmax(dim=1)
            cost = torch.sum(
                softmax_pred_T * logsoftmax(t.view(-1, W * H) / self.tau) -
                softmax_pred_T * logsoftmax(s.view(-1, W * H) / self.tau)) * (self.tau ** 2)
 
            losses.append(cost / (C * N))
        loss = sum(losses)
 
        return loss
 
 
class MGDLoss(nn.Module):
    def __init__(self, channels_s, channels_t, alpha_mgd=0.00002, lambda_mgd=0.65):
        super(MGDLoss, self).__init__()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.alpha_mgd = alpha_mgd
        self.lambda_mgd = lambda_mgd
 
        self.generation = [
            nn.Sequential(
                nn.Conv2d(channel, channel, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(channel, channel, kernel_size=3, padding=1)).to(device) for channel in channels_t
        ]
 
    def forward(self, y_s, y_t):
        """Forward computation.
        Args:
            y_s (list): The student model prediction with
                shape (N, C, H, W) in list.
            y_t (list): The teacher model prediction with
                shape (N, C, H, W) in list.
        Return:
            torch.Tensor: The calculated loss value of all stages.
        """
        assert len(y_s) == len(y_t)
        losses = []
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            assert s.shape == t.shape
            losses.append(self.get_dis_loss(s, t, idx) * self.alpha_mgd)
        loss = sum(losses)
        return loss
 
    def get_dis_loss(self, preds_S, preds_T, idx):
        loss_mse = nn.MSELoss(reduction='sum')
        N, C, H, W = preds_T.shape
 
        device = preds_S.device
        mat = torch.rand((N, 1, H, W)).to(device)
        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)
 
        masked_fea = torch.mul(preds_S, mat)
        new_fea = self.generation[idx](masked_fea)
        dis_loss = loss_mse(new_fea, preds_T) / N
        return dis_loss
 
 
class Distill_LogitLoss:
    def __init__(self, p, t_p, alpha=0.25):
        t_ft = torch.cuda.FloatTensor if t_p[0].is_cuda else torch.Tensor
        self.p = p
        self.t_p = t_p
        self.logit_loss = t_ft([0])
        self.DLogitLoss = nn.MSELoss(reduction="none")
        self.bs = p[0].shape[0]
        self.alpha = alpha
 
    def __call__(self):
        # per output
        assert len(self.p) == len(self.t_p)
        for i, (pi, t_pi) in enumerate(zip(self.p, self.t_p)):  # layer index, layer predictions
            assert pi.shape == t_pi.shape
            self.logit_loss += torch.mean(self.DLogitLoss(pi, t_pi))
        return self.logit_loss[0] * self.alpha
 
 
def get_fpn_features(x, model, fpn_layers=[15, 18, 21]):
    y, fpn_feats = [], []
    with torch.no_grad():
        model = de_parallel(model)
        module_list = model.model[:-1] if hasattr(model, "model") else model[:-1]
        for m in module_list:
            # if not from previous layer
            if m.f != -1:
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            x = m(x)
            y.append(x if m.i in model.save else None)  # save output
            if m.i in fpn_layers:
                fpn_feats.append(x)
    return fpn_feats
 
def get_channels(model, fpn_layers=[15, 18, 21]):
    y, out_channels = [], []
    p = next(model.parameters())
    x = torch.zeros((1, 3, 64, 64), device=p.device)
    with torch.no_grad():
        model = de_parallel(model)
        module_list = model.model[:-1] if hasattr(model, "model") else model[:-1]
 
        for m in module_list:
            # if not from previous layer
            if m.f != -1:
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            x = m(x)
            y.append(x if m.i in model.save else None)  # save output
            if m.i in fpn_layers:
                out_channels.append(x.shape[1])
    return out_channels
 
 
class FeatureLoss(nn.Module):
    def __init__(self, channels_s, channels_t, distiller='cwd'):
        super(FeatureLoss, self).__init__()
 
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.align_module = nn.ModuleList([
            nn.Conv2d(channel, tea_channel, kernel_size=1, stride=1, padding=0).to(device)
            for channel, tea_channel in zip(channels_s, channels_t)
        ])
        self.norm = [
            nn.BatchNorm2d(tea_channel, affine=False).to(device)
            for tea_channel in channels_t
        ]
 
        if distiller == 'mimic':
            self.feature_loss = MimicLoss(channels_s, channels_t)
 
        elif distiller == 'mgd':
            self.feature_loss = MGDLoss(channels_s, channels_t)
 
        elif distiller == 'cwd':
            self.feature_loss = CWDLoss(channels_s, channels_t)
        else:
            raise NotImplementedError
 
    def forward(self, y_s, y_t):
        assert len(y_s) == len(y_t)
        tea_feats = []
        stu_feats = []
 
        for idx, (s, t) in enumerate(zip(y_s, y_t)):
            s = self.align_module[idx](s)
            s = self.norm[idx](s)
            t = self.norm[idx](t)
            tea_feats.append(t)
            stu_feats.append(s)
 
        loss = self.feature_loss(stu_feats, tea_feats)
        return loss
```

* * *

### 2.2 ä¿®æ”¹äºŒ

ä¸‹é¢çš„ä»£ç æˆ‘ä»¬æ‰¾åˆ°æ–‡ä»¶'ultralytics/engine/trainer.py'æŒ‰ç…§æˆ‘çš„å›¾ç‰‡å†…å®¹å¤åˆ¶ç²˜è´´åˆ°æŒ‡å®šä½ç½®å³å¯ï¼**æ ¹æ®ä¸‹å›¾è¿›è¡Œä¿®æ”¹->**

**åœ¨è¯¥æ–‡ä»¶çš„å¼€å¤´æˆ‘ä»¬å…ˆæ·»åŠ ä¸¤è¡Œæ¨¡å—çš„å¯¼å…¥ä»£ç ï¼Œ**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/75c50bcf7da0b274a82980957568a607.png)

```python
from ultralytics.utils import IterableSimpleNamespace
from ultralytics.utils.AddLoss import get_fpn_features, Distill_LogitLoss, de_parallel, get_channels, FeatureLoss
```





![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/9e0dd4f62fb8bc7cf0d58207febd24d2.png)â€‹

```python
        #------------------------------Add-Param-Start---------------
        self.featureloss = 0
        self.logitloss = 0
        self.teacherloss = 0
        self.distillloss =None
        self.model_t = overrides.get("model_t", None)
        self.distill_feat_type = "cwd"  # "cwd","mgd","mimic"
        self.distillonline = False  # False or True
        self.logit_loss = False # False or True
        self.distill_layers = [2, 4, 6, 8, 12, 15, 18, 21]
        # ------------------------------Add-Param-End-----------------
```

* * *

### 2.3 ä¿®æ”¹ä¸‰

æŒ‰ç…§å›¾ç‰‡è¿›è¡Œä¿®æ”¹å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/8a0944a19191b16fb6377031860965e5.png)â€‹

```python
        if self.model_t is not None:
            for k, v in self.model_t.model.named_parameters():
                v.requires_grad = True
            self.model_t = self.model_t.to(self.device)
```

* * *

### 2.4 ä¿®æ”¹å››Â 

**æŒ‰ç…§å›¾ç‰‡è¿›è¡Œä¿®æ”¹å³å¯ã€‚**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/91c986bedf8396dde7eb0d932a4c4474.png)â€‹

```python
            if self.model_t is not None:
                self.model_t = nn.parallel.DistributedDataParallel(self.model_t, device_ids=[RANK])
```



* * *

### 2.5 ä¿®æ”¹äº”Â 

æ­¤å¤„çš„ä¿®æ”¹å’Œä¸Šé¢æœ‰äº›ä¸ä¸€æ ·ï¼Œä¸Šé¢çš„ä»£ç éƒ½æ˜¯æ·»åŠ ï¼Œæ­¤å¤„çš„ä»£ç ä¸ºæ›¿æ¢ã€‚

```
        self.optimizer = self.build_optimizer(model=self.model,
                                              model_t=self.model_t,
                                              distillloss=self.distillloss,
                                              distillonline=self.distillonline,
                                              name=self.args.optimizer,
                                              lr=self.args.lr0,
                                              momentum=self.args.momentum,
                                              decay=weight_decay,
                                              iterations=iterations)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/269369a15060c4a7e978184ecc5f72ca.png)â€‹

* * *

### 2.6 ä¿®æ”¹å…­

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

```python
        self.model = de_parallel(self.model)
        if self.model_t is not None:
            self.model_t = de_parallel(self.model_t)
            self.channels_s = get_channels(self.model,self.distill_layers)
            self.channels_t = get_channels(self.model_t,self.distill_layers)
            self.distillloss = FeatureLoss(channels_s=self.channels_s, channels_t=self.channels_t, distiller= self.distill_feat_type)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/bffa91ba199491eea639b992b54319f7.png)â€‹

* * *

### 2.7 ä¿®æ”¹ä¸ƒÂ 

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

```python
            if self.model_t is not None:
                self.model_t.eval()
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/ce70466a6fecf9192f58f54e010a35ed.png)â€‹

* * *

### 2.8 ä¿®æ”¹å…«Â 

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

```
                    pred_s= self.model(batch['img'])
                    stu_features = get_fpn_features(batch['img'], self.model,fpn_layers=self.distill_layers)
```



![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/396a0574ac2a105925cb7de38aa6b875.png)

* * *

### 2.8 ä¿®æ”¹ä¹

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/f824b6a1d28753225a30708809dfc7a0.png)â€‹

```
                    if self.model_t is not None:
                        distill_weight = ((1 - math.cos(i * math.pi / len(self.train_loader))) / 2) * (0.1 - 1) + 1
                        with torch.no_grad():
                            pred_t_offline = self.model_t(batch['img'])
                            tea_features = get_fpn_features(batch['img'], self.model_t,
                                                            fpn_layers=self.distill_layers)  # forward
                            self.featureloss = self.distillloss(stu_features, tea_features) * distill_weight
                            self.loss += self.featureloss
 
                        if self.distillonline:
                            self.model_t.train()
                            pred_t_online = self.model_t(batch['img'])
                            for p in pred_t_online:
                                p = p.detach()
                            if i == 0 and epoch == 0:
                                self.model_t.args["box"] = self.model.args.box
                                self.model_t.args["cls"] = self.model.args.cls
                                self.model_t.args["dfl"] = self.model.args.dfl
                                self.model_t.args = IterableSimpleNamespace(**self.model_t.args)
                            self.teacherloss, _ = self.model_t(batch, pred_t_online)
 
                            if RANK != -1:
                                self.teacherloss *= world_size
                            self.loss += self.teacherloss
 
                        if self.logit_loss:
                            if not self.distillonline:
                                distill_logit = Distill_LogitLoss(pred_s, pred_t_offline)
                            else:
                                distill_logit = Distill_LogitLoss(pred_s, pred_t_online)
                            self.logitloss = distill_logit()
                            self.loss += self.logitloss
```



* * *

### 2.9 ä¿®æ”¹å

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/34fb82b4b0a045659a84dc210597c2c9.png)â€‹

```
                mem = f"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G"  # (GB)
                loss_len = self.tloss.shape[0] if len(self.tloss.shape) else 1
                losses = self.tloss if loss_len > 1 else torch.unsqueeze(self.tloss, 0)
                if RANK in {-1, 0}:
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1
                    pbar.set_description(
                        ('%12s' * 2 + '%12.4g' * (5 + loss_length)) %
                        (f'{epoch + 1}/{self.epochs}', mem, * losses, self.featureloss, self.teacherloss, self.logitloss, batch['cls'].shape[0], batch['img'].shape[-1]))
                    self.run_callbacks("on_batch_end")
                    if self.args.plots and ni in self.plot_idx:
                        self.plot_training_samples(batch, ni)
```



* * *

### 2.10 ä¿®æ”¹åä¸€

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

```cobol
, model_t, distillloss, distillonline=False,
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/461f9e7eab2ec673c70a592e1b9880fb.png)â€‹

* * *

### 2.11 ä¿®æ”¹åäºŒ

**ä¿®æ”¹æ•™ç¨‹çœ‹å›¾ç‰‡ï¼**

```
        if model_t is not None and distillonline:
            for v in model_t.modules():
                # print(v)
                if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)
                    g[2].append(v.bias)
                if isinstance(v, bn):  # weight (no decay)
                    g[1].append(v.weight)
                elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
                    g[0].append(v.weight)
 
        if model_t is not None and distillloss is not None:
            for k, v in distillloss.named_modules():
                # print(v)
                if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)
                    g[2].append(v.bias)
                if isinstance(v, bn) or 'bn' in k:  # weight (no decay)
                    g[1].append(v.weight)
                elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
                    g[0].append(v.weight)
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/2aaa34487751a2a75b69e7ae11ebf041.png)â€‹Â 

* * *

### 2.12 ä¿®æ”¹åä¸‰

PSï¼šæ³¨æ„æ­¤å¤„æˆ‘ä»¬æ›´æ¢äº†ä¿®æ”¹çš„æ–‡ä»¶äº†ï¼ï¼

æˆ‘ä»¬æ‰¾åˆ°æ–‡ä»¶'ultralytics/cfg/\_\_init\_\_.py'ï¼ŒæŒ‰ç…§æˆ‘çš„å›¾ç‰‡è¿›è¡Œä¿®æ”¹ï¼

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0cd5a2dfc2166bf627d902dbeb110a7d.png)â€‹

* * *

### 2.13 ä¿®æ”¹åå››

PSï¼šæ³¨æ„æ­¤å¤„æˆ‘ä»¬æ›´æ¢äº†ä¿®æ”¹çš„æ–‡ä»¶äº†ï¼ï¼

æˆ‘ä»¬æ‰¾åˆ°æ–‡ä»¶'ultralytics/models/[yolo](https://so.csdn.net/so/search?q=yolo&spm=1001.2101.3001.7020)/detect/train.py'æŒ‰ç…§å›¾ç‰‡è¿›è¡Œä¿®æ”¹å³å¯ï¼

```
        return ('\n' + '%12s' *
                (7 + len(self.loss_names))) % (
        'Epoch', 'GPU_mem', *self.loss_names, 'dfeaLoss', 'dlineLoss', 'dlogitLoss', 'Instances',
        'Size')
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/149cb45c94ef85a8a984fb67f1fb4a50.png)â€‹

* * *

### 2.14 ä¿®æ”¹åäº”

PSï¼šæ³¨æ„æ­¤å¤„æˆ‘ä»¬æ›´æ¢äº†ä¿®æ”¹çš„æ–‡ä»¶äº†ï¼ï¼

æˆ‘ä»¬æ‰¾åˆ°æ–‡ä»¶'ultralytics/engine/model.py'æŒ‰ç…§å›¾ç‰‡è¿›è¡Œä¿®æ”¹å³å¯ï¼

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/0e61fc3af46c27799e142c941ef55ed7.png)â€‹Â 

**åˆ°æ­¤å¤„å°±ä¿®æ”¹å®Œæˆäº†ï¼Œå‰©ä¸‹çš„å°±æ˜¯å¦‚ä½•ä½¿ç”¨è¿›è¡Œæ¨¡å‹è’¸é¦äº†ï¼**Â 

* * *

## ä¸‰ã€ä½¿ç”¨æ•™ç¨‹Â 

æ¨¡å‹è’¸é¦æŒ‡çš„æ˜¯ï¼šç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ³¨æ„æ˜¯è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰å»æ•™å¦ä¸€ä¸ªæ¨¡å‹ï¼ï¼ˆå…¶ä¸­æ•™å¸ˆæ¨¡å‹å¿…é¡»æ˜¯è®­ç»ƒå¥½çš„æƒé‡ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥æ˜¯yamlæ–‡ä»¶ä¹Ÿå¯ä»¥æ˜¯æƒé‡æ–‡ä»¶åœ°å€ï¼‰

åœ¨å¼€å§‹ä¹‹å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬è¿™é‡Œå°±ç”¨YOLOv8lä¸ºä¾‹ï¼Œå…¶æƒé‡æ–‡ä»¶å¯ä»¥å»å®˜æ–¹ä¸‹è½½ã€‚Â 

### 3.1 æ¨¡å‹è’¸é¦ä»£ç 

PSï¼šéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„æ¨¡å‹é…ç½®æ–‡ä»¶éœ€è¦ä¿æŒä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯è¯´ä½ å­¦ç”Ÿæ¨¡å‹å‡è®¾ç”¨äº†BiFPNé‚£ä¹ˆä½ çš„æ•™å¸ˆæ¨¡å‹ä¹Ÿéœ€è¦ç”¨BiFPNå»è®­ç»ƒå¦åˆ™å°±ä¼šæŠ¥é”™ï¼

```
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO
 
if __name__ == '__main__':
    model_t = YOLO(r'weights/yolov8l.pt')  # æ­¤å¤„å¡«å†™æ•™å¸ˆæ¨¡å‹çš„æƒé‡æ–‡ä»¶åœ°å€
 
    model_t.model.model[-1].set_Distillation = True  # ä¸ç”¨ç†ä¼šæ­¤å¤„ç”¨äºè®¾ç½®æ¨¡å‹è’¸é¦
 
    model_s = YOLO(r'ultralytics/cfg/models/v8/yolov8.yaml')  # å­¦ç”Ÿæ–‡ä»¶çš„yamlæ–‡ä»¶ or æƒé‡æ–‡ä»¶åœ°å€
 
    model_s.train(data=r'C:\Users\Administrator\Desktop\Snu77\ultralytics-main\New_GC-DET\data.yaml',  #  å°†dataåé¢æ›¿æ¢ä½ è‡ªå·±çš„æ•°æ®é›†åœ°å€
                cache=False,
                imgsz=640,
                epochs=100,
                single_cls=False,  # æ˜¯å¦æ˜¯å•ç±»åˆ«æ£€æµ‹
                batch=1,
                close_mosaic=10,
                workers=0,
                device='0',
                optimizer='SGD',  # using SGD
                amp=True,  # å¦‚æœå‡ºç°è®­ç»ƒæŸå¤±ä¸ºNanå¯ä»¥å…³é—­amp
                project='runs/train',
                name='exp',
                model_t=model_t.model
                )
```



* * *

### 3.2 å¼€å§‹è’¸é¦Â 

æˆ‘ä»¬å°†è’¸é¦çš„ä»£ç å¤åˆ¶ç²˜è´´åˆ°ä¸€ä¸ªpyæ–‡ä»¶å†…ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/5b2c9311f52d08fc4d304239d309937a.png)â€‹

æˆ‘ä»¬è¿è¡Œè’¸é¦çš„pyæ–‡ä»¶å³å¯ï¼Œæ¨¡å‹å°±ä¼šå¼€å§‹è®­ç»ƒå¹¶ä¸”è’¸é¦ã€‚ä¸‹é¢çš„å›¾ç‰‡å°±æ˜¯æ¨¡å‹å¼€å§‹è®­ç»ƒå¹¶ä¸”è’¸é¦ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¼€å¯äº†è’¸é¦æŸå¤±'dfeaLoss Â  dlineLoss'

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/339ee686982c2e83b90cfd85e2890948.png)â€‹Â 

